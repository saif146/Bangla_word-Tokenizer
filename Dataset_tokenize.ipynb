{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset_tokenize.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1EXTMB6X5ek",
        "outputId": "f882f03c-b8ab-42dd-c63e-b4735860b074"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SNgP40DYTk0"
      },
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzTKol0lYLCE"
      },
      "source": [
        "input_ban_frame = pd.read_excel('/content/drive/MyDrive/Thesis/code/Bangla MSR Video Description Corpus.xlsx')  # Loading text from csv file\n",
        "# input_ban_frame = pd.read_excel(\"/yourpath/input.xlsx\")  # Loading text from excel file\n",
        "ban_column = input_ban_frame['Description'] # Retrieving text from input file\n",
        "#ban_sentence= ban_column.str.cat(sep=', ')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bhJu37ZZ9ZE",
        "outputId": "881321d9-52cd-4501-9a13-6fcb7907cba5"
      },
      "source": [
        "print(len(ban_column))\n",
        "ban_column.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85550\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    ডুবন্ত একটি পাখি একটি কল থেকে প্রবাহিত জলের নি...\n",
              "1                            একটি পাখি ডুবে গোসল করছে।\n",
              "2     একটি পাখি চলমান কলের নীচে চারপাশে ছড়িয়ে পড়ছে।\n",
              "3                            একটি পাখি ডুবে গোসল করছে।\n",
              "4    একটি পাখি ডুবন্ত পানিতে দাঁড়িয়ে আছে যা মুখের...\n",
              "Name: Description, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfMhWT_nZFdE",
        "outputId": "006ea274-da87-4e8c-eeb8-beefa148675e"
      },
      "source": [
        "!pip install cltk==0.1.121\n",
        "from cltk.tokenize.sentence import TokenizeSentence"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cltk==0.1.121\n",
            "  Downloading cltk-0.1.121.tar.gz (625 kB)\n",
            "\u001b[K     |████████████████████████████████| 625 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting gitpython\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from cltk==0.1.121) (3.2.5)\n",
            "Collecting python-crfsuite\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 48.9 MB/s \n",
            "\u001b[?25hCollecting pyuca\n",
            "  Downloading pyuca-1.2-py2.py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from cltk==0.1.121) (3.13)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from cltk==0.1.121) (2019.12.20)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->cltk==0.1.121) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->cltk==0.1.121) (1.15.0)\n",
            "Building wheels for collected packages: cltk\n",
            "  Building wheel for cltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cltk: filename=cltk-0.1.121-py3-none-any.whl size=711644 sha256=4f77ed7e092d83caa44773442ac4b9574f5bcdb50368d63c7996c4c5cdef0a20\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/8c/b8/9901ff1ef48c84c71c52cfba8c67ed20fb2fcf4e190904c9e5\n",
            "Successfully built cltk\n",
            "Installing collected packages: smmap, gitdb, whoosh, pyuca, python-crfsuite, gitpython, cltk\n",
            "Successfully installed cltk-0.1.121 gitdb-4.0.7 gitpython-3.1.24 python-crfsuite-0.9.7 pyuca-1.2 smmap-4.0.0 whoosh-2.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWIIEKcWb5XA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV51cTOcdm4S"
      },
      "source": [
        "captions=ban_column"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH07SGUId0c7",
        "outputId": "be0f7bce-fcb0-4471-f511-dd2ebae9135c"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNOpTAJjeX4p"
      },
      "source": [
        "caption_path =  '/content/drive/MyDrive/Thesis/code/Bangla MSR Video Description Corpus.xlsx'      # directory of the csv file\n",
        "vocab_path = '/content/drive/MyDrive/Thesis/code/bangla_vocab.pkl'       # directory for saving vocabulary object (word to id dictionary)\n",
        "threshold = 3  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpJmp96HfRkz"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va-dvg3ekS_n",
        "outputId": "a5c53fb8-b5cf-449a-8d64-051f4abe06fe"
      },
      "source": [
        "!pip install bnlp_toolkit"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bnlp_toolkit\n",
            "  Downloading bnlp_toolkit-3.1.2-py3-none-any.whl (17 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bnlp_toolkit) (1.19.5)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.7/dist-packages (from bnlp_toolkit) (0.8.2)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting gensim==4.0.1\n",
            "  Downloading gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from bnlp_toolkit) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from bnlp_toolkit) (3.2.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.1->bnlp_toolkit) (5.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->bnlp_toolkit) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (4.62.2)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (0.9.7)\n",
            "Installing collected packages: sklearn-crfsuite, sentencepiece, gensim, bnlp-toolkit\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed bnlp-toolkit-3.1.2 gensim-4.0.1 sentencepiece-0.1.96 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly_-omS-TN9z"
      },
      "source": [
        "from collections import Counter\n",
        "threshold=3"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG6HeA-7T6vT",
        "outputId": "88124230-8000-4252-bd42-031f4963bcc8"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrk-0xbTgom-",
        "outputId": "6d688105-e9de-4c88-a22c-47afec3bb552"
      },
      "source": [
        "def build_vocab(ban_column, threshold):\n",
        "    counter = Counter()\n",
        "    from bnlp import NLTKTokenizer\n",
        "    \n",
        "    bnltk = NLTKTokenizer()\n",
        "    captions=ban_column\n",
        "    for i, caption in enumerate(captions):\n",
        "        word_tokens = bnltk.word_tokenize(str(caption))\n",
        "        #print(word_tokens)\n",
        "        tokens = [word for word in word_tokens if word.isalpha()]\n",
        "        counter.update(tokens)\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(captions)))\n",
        "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "        # Create a vocab wrapper and add some special tokens.\n",
        "    vocab = Vocabulary()\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<start>')\n",
        "    vocab.add_word('<end>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "        # Add the words to the vocabulary.\n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "    return vocab \n",
        "def main(ban_column,vocab_path,threshold):\n",
        "    vocab = build_vocab(ban_column, threshold)\n",
        "    vocab_path = vocab_path\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump(vocab, f)\n",
        "    print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
        "    print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))\n",
        "nltk.download('punkt')\n",
        "main(ban_column, vocab_path,threshold)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[1000/85550] Tokenized the captions.\n",
            "[2000/85550] Tokenized the captions.\n",
            "[3000/85550] Tokenized the captions.\n",
            "[4000/85550] Tokenized the captions.\n",
            "[5000/85550] Tokenized the captions.\n",
            "[6000/85550] Tokenized the captions.\n",
            "[7000/85550] Tokenized the captions.\n",
            "[8000/85550] Tokenized the captions.\n",
            "[9000/85550] Tokenized the captions.\n",
            "[10000/85550] Tokenized the captions.\n",
            "[11000/85550] Tokenized the captions.\n",
            "[12000/85550] Tokenized the captions.\n",
            "[13000/85550] Tokenized the captions.\n",
            "[14000/85550] Tokenized the captions.\n",
            "[15000/85550] Tokenized the captions.\n",
            "[16000/85550] Tokenized the captions.\n",
            "[17000/85550] Tokenized the captions.\n",
            "[18000/85550] Tokenized the captions.\n",
            "[19000/85550] Tokenized the captions.\n",
            "[20000/85550] Tokenized the captions.\n",
            "[21000/85550] Tokenized the captions.\n",
            "[22000/85550] Tokenized the captions.\n",
            "[23000/85550] Tokenized the captions.\n",
            "[24000/85550] Tokenized the captions.\n",
            "[25000/85550] Tokenized the captions.\n",
            "[26000/85550] Tokenized the captions.\n",
            "[27000/85550] Tokenized the captions.\n",
            "[28000/85550] Tokenized the captions.\n",
            "[29000/85550] Tokenized the captions.\n",
            "[30000/85550] Tokenized the captions.\n",
            "[31000/85550] Tokenized the captions.\n",
            "[32000/85550] Tokenized the captions.\n",
            "[33000/85550] Tokenized the captions.\n",
            "[34000/85550] Tokenized the captions.\n",
            "[35000/85550] Tokenized the captions.\n",
            "[36000/85550] Tokenized the captions.\n",
            "[37000/85550] Tokenized the captions.\n",
            "[38000/85550] Tokenized the captions.\n",
            "[39000/85550] Tokenized the captions.\n",
            "[40000/85550] Tokenized the captions.\n",
            "[41000/85550] Tokenized the captions.\n",
            "[42000/85550] Tokenized the captions.\n",
            "[43000/85550] Tokenized the captions.\n",
            "[44000/85550] Tokenized the captions.\n",
            "[45000/85550] Tokenized the captions.\n",
            "[46000/85550] Tokenized the captions.\n",
            "[47000/85550] Tokenized the captions.\n",
            "[48000/85550] Tokenized the captions.\n",
            "[49000/85550] Tokenized the captions.\n",
            "[50000/85550] Tokenized the captions.\n",
            "[51000/85550] Tokenized the captions.\n",
            "[52000/85550] Tokenized the captions.\n",
            "[53000/85550] Tokenized the captions.\n",
            "[54000/85550] Tokenized the captions.\n",
            "[55000/85550] Tokenized the captions.\n",
            "[56000/85550] Tokenized the captions.\n",
            "[57000/85550] Tokenized the captions.\n",
            "[58000/85550] Tokenized the captions.\n",
            "[59000/85550] Tokenized the captions.\n",
            "[60000/85550] Tokenized the captions.\n",
            "[61000/85550] Tokenized the captions.\n",
            "[62000/85550] Tokenized the captions.\n",
            "[63000/85550] Tokenized the captions.\n",
            "[64000/85550] Tokenized the captions.\n",
            "[65000/85550] Tokenized the captions.\n",
            "[66000/85550] Tokenized the captions.\n",
            "[67000/85550] Tokenized the captions.\n",
            "[68000/85550] Tokenized the captions.\n",
            "[69000/85550] Tokenized the captions.\n",
            "[70000/85550] Tokenized the captions.\n",
            "[71000/85550] Tokenized the captions.\n",
            "[72000/85550] Tokenized the captions.\n",
            "[73000/85550] Tokenized the captions.\n",
            "[74000/85550] Tokenized the captions.\n",
            "[75000/85550] Tokenized the captions.\n",
            "[76000/85550] Tokenized the captions.\n",
            "[77000/85550] Tokenized the captions.\n",
            "[78000/85550] Tokenized the captions.\n",
            "[79000/85550] Tokenized the captions.\n",
            "[80000/85550] Tokenized the captions.\n",
            "[81000/85550] Tokenized the captions.\n",
            "[82000/85550] Tokenized the captions.\n",
            "[83000/85550] Tokenized the captions.\n",
            "[84000/85550] Tokenized the captions.\n",
            "[85000/85550] Tokenized the captions.\n",
            "Total vocabulary size: 756\n",
            "Saved the vocabulary wrapper to '/content/drive/MyDrive/Thesis/code/bangla_vocab.pkl'\n"
          ]
        }
      ]
    }
  ]
}